I"<p>Bootstrapping methods are more difficult to combine with FA than are non-bootstrapping methods.</p>

<!-- KaTeX -->
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<h1 id="what-is-function-approximation-">What is function approximation ?</h1>

<h1 id="what-is-the-main-cause-of-diverging-value-function-">What is the main cause of diverging value function ?</h1>

<h1 id="when-can-the-value-function-diverge-">When can the value function diverge ?</h1>

<p><span style="color:blue"><b>The deadly triad</b></span> <a href="http://media.nips.cc/Conferences/2015/tutorialslides/SuttonIntroRL-nips-2015-tutorial.pdf"><em>(exert from Suttonâ€™s slides NIPS 2015 tutorial)</em></a></p>

<p>The risk of divergence arises whenever we combine three things:</p>

<ol>
  <li>
    <p><span style="color:red"><b>Function approximation:</b></span><br />
significantly generalizing from large numbers of examples.</p>
  </li>
  <li>
    <p><span style="color:red"><b>Bootstrapping</b></span><br />
learning value estimates from other value estimates,
as in dynamic programming and temporal-difference learning.</p>
  </li>
  <li>
    <p><span style="color:red"><b>Off-policy learning</b></span><br />
learning about a policy from data not due to that policy,
as in Q-learning, where we learn about the greedy policy from
data with a necessarily more exploratory policy.</p>
  </li>
</ol>

<p>Based on the above the following should converge (always?):</p>

<ul>
  <li>On-policy with any form of Bootstrapping such as TD(0)</li>
</ul>

<h1 id="offon-policy">Off/On-policy</h1>

<ul>
  <li>Value Iteration: off-policy</li>
  <li>Q-learning: off-policy</li>
  <li>Policy Iteration: on-policy</li>
  <li>SARSA: on-policy</li>
</ul>
:ET